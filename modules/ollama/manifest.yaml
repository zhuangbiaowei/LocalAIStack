name: ollama
category: runtime
version: 0.1.0
description: Local LLM inference runtime powered by Ollama
license: MIT

hardware:
  cpu:
    cores_min: 2
  memory:
    ram_min: 4GB
  gpu:
    vram_min: 6GB
    multi_gpu: false

dependencies:
  system:
    - curl
    - ca-certificates

runtime:
  modes:
    - native
  preferred: native

interfaces:
  provides:
    - local_llm_inference
